{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:22:47.533815Z",
     "start_time": "2018-07-17T06:22:47.531647Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from typing import List, Tuple, Dict, Callable, Optional, Any, Sequence, Mapping, NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-17T06:22:48.500114Z",
     "start_time": "2018-07-17T06:22:47.671522Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T06:56:18.120650Z",
     "start_time": "2018-07-12T06:56:18.118277Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.char_to_idx, self.idx_to_char = pickle.load(open('./data/vocab_jis_second.pkl', 'rb'))\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T09:03:22.168737Z",
     "start_time": "2018-07-18T09:03:22.151871Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder:\n",
    "    \n",
    "    def __init__(self, reuse=None):\n",
    "        self.batch_size = 64\n",
    "        self.num_blocks = 4\n",
    "        self.num_units = 512\n",
    "        self.num_outputs = 2\n",
    "        self.num_inner_units = 2048\n",
    "        self.num_heads = 8\n",
    "        self.vocab_size = 6594\n",
    "        \n",
    "        self._create_placeholder()\n",
    "        self._create_model(reuse)\n",
    "        \n",
    "    def _create_placeholder(self):\n",
    "        self.is_training = tf.placeholder(shape=(), dtype=tf.bool, name='is_training')\n",
    "        self.inputs_data = tf.placeholder(shape=[None, None], name='inputs_data', dtype=tf.int32)  # batch_size x max_length\n",
    "        #self.targets_data = tf.placeholder(shape=[None, self.num_outputs], name='targets_data', dtype=tf.int32)  # batch_size x num_outputs\n",
    "        \n",
    "    def _create_model(self, reuse):\n",
    "        with tf.variable_scope('transformer', reuse=reuse):\n",
    "            inputs = self.inputs_data\n",
    "            embedded_inputs = self._embedding(inputs)  # [batch_size, max_length, embedded_size]\n",
    "            embedded_inputs += self._positional_encoding(inputs)  # [batch_size, max_length, embedded_size]\n",
    "            encoded_query = self._encode(embedded_inputs)\n",
    "            self.outputs_prob = self._dense(encoder_query, self.num_outputs, 'outputs', tf.nn.softmax)\n",
    "            self.predicted_class = tf.argmax(self.outputs_prob, axis=-1)\n",
    "        \n",
    "    def _encode(self,\n",
    "                    inputs,  # [batch_size, max_length, num_units]\n",
    "                    scope: str='encoder'):\n",
    "        with tf.variable_scope(scope):\n",
    "            queries = inputs\n",
    "            encoded_queries = tf.get_variable('encoded_queries', \n",
    "                                                      dtype=myfloat, \n",
    "                                                      shape=[1, self.num_units], \n",
    "                                                      initializer=tf.contrib.layers.xavier_initializer())  # [batch_size, 1, num_units]\n",
    "            encoder_queries = tf.tile(tf.expand_dims(encoder_query, 0), [tf.shape(inputs)[0], 1, 1])\n",
    "            for i in range(self.num_blocks):\n",
    "                with tf.variable_scope('block_{}'.format(i)):\n",
    "                    original_queries = queries\n",
    "                    queries = self._multihead_attention(\n",
    "                        keys=queries, \n",
    "                        queries=queries, \n",
    "                        num_units=self.num_units, \n",
    "                        num_heads=self.num_heads, \n",
    "                        causality=False, \n",
    "                        reuse=None\n",
    "                    )\n",
    "                    queries += original_queries\n",
    "                    queries = self._normalize(queries, scope='mh_normalize')\n",
    "                    \n",
    "                    original_queries = encoded_queries\n",
    "                    encoded_queries = self._multihead_attention(\n",
    "                        keys=queries,\n",
    "                        queries=encoded_queries,\n",
    "                        num_units=self.num_units,\n",
    "                        num_heads=self.num_heads,\n",
    "                        causality=False,\n",
    "                        reuse=None\n",
    "                    )\n",
    "                    encoded_queries += original_queries\n",
    "                    encoded_queries = self.normalize(encoded_queries)\n",
    "\n",
    "                    original_queries = queries\n",
    "                    queries = self._feedforward(queries, [self.num_inner_units, self.num_units])\n",
    "                    queries += original_queries\n",
    "                    queries = self._normalize(queries, scope='ff_normalize')\n",
    "        return queries\n",
    "    \n",
    "    def _multihead_attention(self,\n",
    "                           keys,  # [batch_size, max_length, embedded_size]\n",
    "                           queries,\n",
    "                           num_units: int,\n",
    "                           num_heads: int=8,\n",
    "                           causality: bool=False,\n",
    "                           scope: str='multihead_attention',\n",
    "                           reuse: bool=None):\n",
    "        with tf.variable_scope(scope):\n",
    "            num_heads_units = num_units / num_heads\n",
    "            keys = self._dense(keys, num_units, 'keys')  # [batch_size, max_length, num_units]\n",
    "            values = self._dense(keys, num_units, 'values')\n",
    "            queries = self._dense(queries, num_units, 'queries')\n",
    "\n",
    "            mh_keys = tf.concat(tf.split(keys, num_heads, axis=2), axis=0)  # [batch_size*num_heads, max_length, num_units/num_heads]\n",
    "            mh_values = tf.concat(tf.split(values, num_heads, axis=2), axis=0)\n",
    "            mh_queries = tf.concat(tf.split(queries, num_heads, axis=2), axis=0)\n",
    "\n",
    "            key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # [batch_size, max_length]\n",
    "            key_masks = tf.tile(key_masks, [num_heads, 1])  # [batch_size*num_heads, max_length]\n",
    "            key_masks = tf.tile(tf.expand_dims(key_masks, 1), [1, tf.shape(queries)[1], 1])  # [batch_size*num_heads, max_length, max_length]\n",
    "\n",
    "            outputs = tf.matmul(mh_queries, tf.transpose(mh_keys, [0, 1, 2]))  # [batch_size*num_heads, max_length, max_length]\n",
    "            outputs = outputs / (num_heads_units)**0.5\n",
    "\n",
    "            paddings = tf.ones_like(outputs)*(-2**32+1)\n",
    "            outputs = tf.where(tf.equal(key_masks, 0), paddings, outputs)\n",
    "\n",
    "            outputs = tf.nn.softmax(outputs)\n",
    "            outputs = tf.matmul(outputs, mh_values)  # [batch_size*num_heads, max_length, num_units/num_heads]\n",
    "\n",
    "            outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2)  # [batch_size, max_length, num_units]\n",
    "\n",
    "            outputs = self._dense(outputs, num_units, 'output')\n",
    "        return outputs\n",
    "    \n",
    "    def _feedforward(self,\n",
    "                        inputs,\n",
    "                        num_units: List[int], # [num_layers, num_units]\n",
    "                        scope: str='feedforward',\n",
    "                        reuse: bool=None):\n",
    "        with tf.variable_scope(scope):\n",
    "            layer = inputs\n",
    "            for (i, units) in enumerate(num_units):\n",
    "                layer = self._dense(layer, units, 'dense_{}'.format(i), tf.nn.relu)\n",
    "        return layer\n",
    "    \n",
    "    def _positional_encoding(self,\n",
    "                                inputs,\n",
    "                                is_zero_pad: bool=True,\n",
    "                                scope: str='positional_encoding'):\n",
    "        outputs = tf.to_float(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    def _embedding(self,\n",
    "                     inputs,\n",
    "                     is_zero_pad: bool=True,\n",
    "                     is_scale: bool=True,\n",
    "                     scope: str='embedding',\n",
    "                     reuse: bool=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            lookup_table = tf.get_variable(\n",
    "                'lookup_table', \n",
    "                shape=[self.vocab_size, self.num_units],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.contrib.layers.xavier_initializer()\n",
    "            )\n",
    "            if is_zero_pad:\n",
    "                lookup_table = tf.concat((tf.zeros(shape=[1, self.num_units]), lookup_table[1:, :]), 0)\n",
    "            embedded = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "            if is_scale:\n",
    "                embedded = embedded * self.num_units ** 0.5\n",
    "        return embedded\n",
    "    \n",
    "    def _normalize(self,\n",
    "                     inputs,\n",
    "                     epsilon=1e-8,\n",
    "                     scope='normalize',\n",
    "                     reuse: bool=None):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            param_dim = inputs.get_shape()[-1]\n",
    "            mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n",
    "            \n",
    "            beta = tf.get_variable('beta', initializer=tf.zeros([param_dim]))\n",
    "            gamma = tf.get_variable('gamma', initializer=tf.ones([param_dim]))\n",
    "            normalized = (inputs - mean)/((variance+epsilon) **0.5)\n",
    "            normalized = normalized * gamma + beta\n",
    "        return normalized\n",
    "    \n",
    "    def _dense(self,\n",
    "                 inputs,\n",
    "                 num_units: int,\n",
    "                 scope: str,\n",
    "                 activation=None,\n",
    "                 dropout_rate: Optional[float]=None):\n",
    "        with tf.variable_scope(scope):\n",
    "            layer = tf.layers.dense(inputs, num_units, activation, name='dense')\n",
    "            if dropout_rate:\n",
    "                layer = tf.layers.dropout(layer, dropout_rate, training=self.is_training, name='dropout')\n",
    "        return layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-18T09:03:38.407034Z",
     "start_time": "2018-07-18T09:03:37.421872Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = TransformerEncoder()\n",
    "    model.outputs_prob\n",
    "    model.predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
