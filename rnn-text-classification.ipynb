{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:51.438146Z",
     "start_time": "2018-08-05T09:42:50.808783Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:51.441375Z",
     "start_time": "2018-08-05T09:42:51.439302Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import NamedTuple, List, Dict, Tuple\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:51.657516Z",
     "start_time": "2018-08-05T09:42:51.530483Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(NamedTuple):\n",
    "    num_units: int = 512\n",
    "    num_layers: int = 6\n",
    "    embedding_size = 512\n",
    "    batch_size: int = 128\n",
    "    max_length: int = 50\n",
    "    dropout_in_rate: float = 0.1\n",
    "    dropout_out_rate: float = 0.2\n",
    "    learning_rate: float = 0.001\n",
    "    grad_clip: float = 5.0\n",
    "    is_layer_norm: bool = False\n",
    "    checkpoint_dir = './checkpoints/'\n",
    "    data_path: str = './data/'\n",
    "    log_dir = './logs/'\n",
    "    \n",
    "    def to_log_dir(self) -> str:\n",
    "        return self.log_dir + 'layers={}/units={}/lr={}'.format(self.num_layers, self.num_units, self.learning_rate)\n",
    "    \n",
    "    def to_ckpt_path(self) -> str:\n",
    "        layernorm = 'T' if self.is_layer_norm else 'F'\n",
    "        return self.checkpoint_dir + 'l{}_u{}_lr{}_clip_ln{}_model.ckpt'.format(self.num_layers, self.num_units, self.learning_rate, layernorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:51.796698Z",
     "start_time": "2018-08-05T09:42:51.663135Z"
    }
   },
   "outputs": [],
   "source": [
    "units = [512, 1024]\n",
    "layers = [4]\n",
    "lrs = [0.001]\n",
    "configs = []\n",
    "for l in layers:\n",
    "    for u in units:\n",
    "        for lr in lrs:\n",
    "            configs.append(Config(num_layers=l, num_units=u, learning_rate=lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:51.979142Z",
     "start_time": "2018-08-05T09:42:51.802128Z"
    }
   },
   "outputs": [],
   "source": [
    "class PTBDataSource:\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        train_path = os.path.join(self.config.data_path, 'ptb.train.txt')\n",
    "        test_path = os.path.join(self.config.data_path, 'ptb.test.txt')\n",
    "        valid_path = os.path.join(self.config.data_path, 'ptb.valid.txt')\n",
    "        \n",
    "        self._word_to_id = self._create_tokenizer(train_path)\n",
    "        self._id_to_word = {v: k for k, v in self._word_to_id.items()}\n",
    "        \n",
    "        self.train = self._create_data(train_path)\n",
    "        self.test = self._create_data(test_path)\n",
    "        self.valid = self._create_data(valid_path)\n",
    "        \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.train)\n",
    "        \n",
    "    def feed_dict_list(self, model):\n",
    "        num_batch = len(self.train) // self.config.batch_size\n",
    "        batch_list = []\n",
    "        inputs, inputs_length, target_ids = self._make_feed_list(self.train)\n",
    "        \n",
    "        # batch_sizeに分ける\n",
    "        for i in range(num_batch):\n",
    "            index_from = i * self.config.batch_size\n",
    "            index_to = (i + 1) * self.config.batch_size\n",
    "            batch_range = range(index_from, index_to)\n",
    "            fd = {\n",
    "                model.inputs: inputs[batch_range],\n",
    "                model.inputs_length: inputs_length[batch_range],\n",
    "                model.target_ids: target_ids[batch_range]\n",
    "            }\n",
    "            batch_list.append(fd)\n",
    "        return batch_list\n",
    "    \n",
    "    def feed_test_list(self, model):\n",
    "        test_data = random.sample(self.test, 256)\n",
    "        inputs, inputs_length, target_ids = self._make_feed_list(test_data)\n",
    "        fd = {\n",
    "            model.inputs: inputs,\n",
    "            model.inputs_length: inputs_length,\n",
    "            model.target_ids: target_ids\n",
    "        }\n",
    "        return fd\n",
    "\n",
    "    def _make_feed_list(self, data):\n",
    "        inputs = []\n",
    "        inputs_length = []\n",
    "        target_ids = []\n",
    "        for (i, sentence) in enumerate(data):\n",
    "            for j in range(len(sentence)-1):\n",
    "                inputs_words = sentence[:j+1][-self.config.max_length:] \n",
    "                inputs.append(inputs_words + [0] * (self.config.max_length - len(inputs_words)))\n",
    "                inputs_length.append(len(inputs_words))\n",
    "                target_ids.append(sentence[j+1])\n",
    "        inputs = np.array(inputs)\n",
    "        inputs_length = np.array(inputs_length)\n",
    "        target_ids = np.array(target_ids)\n",
    "        \n",
    "        return inputs, inputs_length, target_ids\n",
    "\n",
    "    def _read_all_words(self, path) -> List[str]:\n",
    "        with open(path, 'r') as f:\n",
    "            return f.read().replace('\\n', '<eos>').split()\n",
    "        \n",
    "    def _read_sentences(self, path) -> List[List[str]]:\n",
    "        with open(path, 'r') as f:\n",
    "            sentences = f.read().split('\\n')\n",
    "            return [sentence.split() for sentence in sentences]\n",
    "\n",
    "    def _create_tokenizer(self, path: str):\n",
    "        data = self._read_all_words(path)\n",
    "        counter = Counter(data)\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "        words, _ = list(zip(*sorted_counter))\n",
    "        word_to_id = dict(zip(words, range(1, len(words)+1)))\n",
    "        return word_to_id\n",
    "        \n",
    "    def _get_id_from_word(self, word: str) -> int:\n",
    "        return self._word_to_id.get(word, self.unk_id)\n",
    "    \n",
    "    def _sentence_to_id_list(self, sentence: List[str]) -> List[int]:\n",
    "        return [self._get_id_from_word(word) for word in sentence]\n",
    "    \n",
    "    def _get_word_from_id(self, word_id: int) -> str:\n",
    "        return self._id_to_word.get(word_id, self.unk_str)\n",
    "    \n",
    "    def _create_data(self, path: str):\n",
    "        return [self._sentence_to_id_list(sentence) for sentence in self._read_sentences(path)]\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def unk_id(self) -> int:\n",
    "        return self._word_to_id.get('<unk>', self.pad_id)\n",
    "    \n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._word_to_id.get('<eos>', self.pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:52.969799Z",
     "start_time": "2018-08-05T09:42:52.913259Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, config: Config, vocab_size):\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "        self._create_placeholder()\n",
    "        self._create_model()\n",
    "        self.loss = self._create_loss()\n",
    "        self.accuracy = self._create_acc()\n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        self.is_training = tf.placeholder(shape=(), dtype=tf.bool, name='is_training')\n",
    "        self.inputs = tf.placeholder(shape=[None, self.config.max_length], dtype=tf.int32, name='inputs')\n",
    "        self.inputs_length = tf.placeholder(shape=[None], dtype=tf.int32, name='inputs_length')\n",
    "        self.target_ids = tf.placeholder(shape=[None], dtype=tf.int32, name='target_ids')\n",
    "    \n",
    "    def _create_model(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        embedded_inputs = self._embedding(self.inputs)\n",
    "        _, encoder_state = self._encode(embedded_inputs)\n",
    "        # encoder_state = tf.layers.dense(encoder_state, num_units, activation=tf.nn.relu, name='hidden_layer')\n",
    "        self.outputs_logits = tf.layers.dense(encoder_state, self.vocab_size, name='outputs_layer')\n",
    "        self.predicted_id = tf.to_int32(tf.argmax(self.outputs_logits, axis=-1))\n",
    "        \n",
    "    def _create_loss(self):\n",
    "        is_target = tf.to_float(tf.not_equal(self.target_ids, 0))\n",
    "        target_ids_one_hot = tf.one_hot(self.target_ids, self.vocab_size)\n",
    "        target_ids_smoothed = self._label_smoothing(target_ids_one_hot)\n",
    "        cross_ent = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.outputs_logits, labels=target_ids_smoothed)\n",
    "        return tf.reduce_sum(cross_ent * is_target) / tf.reduce_sum(is_target)\n",
    "        \n",
    "    def _create_acc(self):\n",
    "        return tf.reduce_mean(tf.to_float(tf.equal(self.target_ids, self.predicted_id)))\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        lookup_table = tf.get_variable('lookup_table', shape=[self.vocab_size, self.config.embedding_size], dtype=tf.float32)\n",
    "        embedded_inputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        return embedded_inputs\n",
    "    \n",
    "    def _encode(self, embedded_inputs):\n",
    "        outputs, final_state = self._bidirectional_cell(\n",
    "            embedded_inputs,\n",
    "            self.config.num_layers,\n",
    "            self.config.num_units,\n",
    "            self.config.dropout_in_rate,\n",
    "            self.config.dropout_out_rate\n",
    "        )\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _bidirectional_cell(self, inputs, num_layers, num_units, dropout_in_rate, dropout_out_rate):\n",
    "        cell_fw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_fw')\n",
    "        cell_bw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_bw')\n",
    "        (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            inputs=inputs,\n",
    "            sequence_length=self.inputs_length,\n",
    "            dtype=tf.float32,\n",
    "            scope='bidirectional_cells')\n",
    "        outputs = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        final_state = tf.reduce_sum([fw_state, bw_state], axis=0)\n",
    "        final_state = tf.concat(tf.unstack(final_state, axis=0), axis=-1)\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _gru(self, num_layers: int, num_units: int, dropout_in_rate: float, dropout_out_rate: float, name: str):\n",
    "        cells = []\n",
    "        for l in range(num_layers):\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units, tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(), name=name)\n",
    "            if l == 0:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1-dropout_in_rate)\n",
    "            if l == num_layers-1:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1-dropout_out_rate)\n",
    "            cells.append(cell)\n",
    "        return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    def _label_smoothing(self, inputs, epsilon: float=0.1):\n",
    "        feature_dim = inputs.get_shape().as_list()[-1]\n",
    "        return (1-epsilon) * inputs + (epsilon / feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:53.063887Z",
     "start_time": "2018-08-05T09:42:52.971533Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:53.166373Z",
     "start_time": "2018-08-05T09:42:53.069239Z"
    }
   },
   "outputs": [],
   "source": [
    "def start():\n",
    "    with tf.Graph().as_default():\n",
    "        now = datetime.now()\n",
    "        logdir = now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "        datasource = PTBDataSource(config)\n",
    "\n",
    "        rnn = RNN(config, datasource.vocab_size)\n",
    "        optimizer = tf.train.AdamOptimizer(config.learning_rate)\n",
    "        train_vars = tf.trainable_variables()\n",
    "        gradients = tf.gradients(rnn.loss, train_vars)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, config.grad_clip)\n",
    "        train_op = optimizer.apply_gradients(zip(clipped_gradients, train_vars), global_step=rnn.global_step)\n",
    "        with tf.name_scope('training'):\n",
    "            s_loss = tf.summary.scalar('loss', rnn.loss)\n",
    "            s_acc = tf.summary.scalar('accuracy', rnn.accuracy)\n",
    "        with tf.name_scope('test'):\n",
    "            test_s_acc = tf.summary.scalar('accuracy', rnn.accuracy)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            writer = tf.summary.FileWriter(config.to_log_dir() + '/' + logdir, sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_epoch):\n",
    "                start = time.time()\n",
    "                datasource.shuffle()\n",
    "                batch_list = datasource.feed_dict_list(rnn)\n",
    "                losses = []\n",
    "                accuracies = []\n",
    "                for (j, fd) in enumerate(batch_list):\n",
    "                    loss, acc, _, smr_loss, smr_acc, step = sess.run([rnn.loss, rnn.accuracy, train_op, s_loss, s_acc, rnn.global_step], feed_dict=fd)\n",
    "                    losses.append(loss)\n",
    "                    accuracies.append(acc)\n",
    "                    writer.add_summary(smr_loss, step)\n",
    "                    writer.add_summary(smr_acc, step)\n",
    "                    if j % 100 == 0:\n",
    "                        #print('loss: {:.3f}, acc: {:.3f}'.format(loss, acc))\n",
    "                        inference(sess, rnn, datasource, writer, test_s_acc, step)\n",
    "                elapsed = time.time() - start\n",
    "                print('epoch {}/{} finished, {} step, elapsed {} sec. average loss: {:.3f}, average accuracy: {:.3f}'.format(i+1, num_epoch, step, elapsed, np.average(losses), np.average(accuracies)))\n",
    "                # loss が nan なら 飛ばす\n",
    "                if math.isnan(np.average(losses)):\n",
    "                    print('loss is nan')\n",
    "                    break\n",
    "                saver.save(sess, config.to_ckpt_path(), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T09:42:53.281705Z",
     "start_time": "2018-08-05T09:42:53.171050Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(sess, model, datasource, writer, s_acc, step):\n",
    "    with tf.name_scope('inference'):\n",
    "        test_list = datasource.feed_test_list(model)\n",
    "        acc, smr_acc = sess.run([model.accuracy, s_acc], feed_dict=test_list)\n",
    "        writer.add_summary(smr_acc, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-05T09:42:50.818Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kentaro.nakanishi/.local/share/virtualenvs/rnn-text-classification-w9qS2Sz3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/kentaro.nakanishi/.local/share/virtualenvs/rnn-text-classification-w9qS2Sz3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "epoch 1/200 finished, 327 step, elapsed 63.05414271354675 sec. average loss: 7.291, average accuracy: 0.066\n",
      "epoch 2/200 finished, 655 step, elapsed 62.784444093704224 sec. average loss: 6.763, average accuracy: 0.122\n",
      "epoch 3/200 finished, 983 step, elapsed 64.81405925750732 sec. average loss: 6.470, average accuracy: 0.148\n",
      "epoch 4/200 finished, 1311 step, elapsed 64.68730759620667 sec. average loss: 6.336, average accuracy: 0.161\n",
      "epoch 5/200 finished, 1639 step, elapsed 66.68140697479248 sec. average loss: 6.196, average accuracy: 0.175\n",
      "epoch 6/200 finished, 1967 step, elapsed 65.44995546340942 sec. average loss: 6.112, average accuracy: 0.184\n",
      "epoch 7/200 finished, 2295 step, elapsed 65.29462242126465 sec. average loss: 6.018, average accuracy: 0.190\n",
      "epoch 8/200 finished, 2623 step, elapsed 64.58694672584534 sec. average loss: 5.995, average accuracy: 0.193\n",
      "epoch 9/200 finished, 2951 step, elapsed 65.00166845321655 sec. average loss: 5.890, average accuracy: 0.204\n",
      "epoch 10/200 finished, 3279 step, elapsed 65.4075448513031 sec. average loss: 5.874, average accuracy: 0.203\n",
      "epoch 11/200 finished, 3607 step, elapsed 65.55463457107544 sec. average loss: 5.762, average accuracy: 0.212\n",
      "epoch 12/200 finished, 3935 step, elapsed 64.85415387153625 sec. average loss: 5.706, average accuracy: 0.218\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:18:31.466637Z",
     "start_time": "2018-08-05T08:18:27.913665Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
