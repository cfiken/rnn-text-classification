{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:24.624346Z",
     "start_time": "2018-09-02T13:24:23.985768Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:24.627379Z",
     "start_time": "2018-09-02T13:24:24.625466Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import NamedTuple, List, Dict, Tuple\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:24.712364Z",
     "start_time": "2018-09-02T13:24:24.628616Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.config import Config\n",
    "from data_loader.ptb_datasource import PTBDataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:24.793654Z",
     "start_time": "2018-09-02T13:24:24.713491Z"
    }
   },
   "outputs": [],
   "source": [
    "units = [512]\n",
    "layers = [2]\n",
    "lrs = [0.001]\n",
    "configs = []\n",
    "for l in layers:\n",
    "    for u in units:\n",
    "        for lr in lrs:\n",
    "            configs.append(Config(num_layers=l, num_units=u, learning_rate=lr, log_dir='./logs/rnn/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:24.947379Z",
     "start_time": "2018-09-02T13:24:24.799054Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, config: Config, vocab_size):\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "        self._create_placeholder()\n",
    "        self._create_model()\n",
    "        self.loss = self._create_loss()\n",
    "        self.accuracy = self._create_acc()\n",
    "        self.perplexity = self._create_perplexity()\n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        self.is_training = tf.placeholder(shape=(), dtype=tf.bool, name='is_training')\n",
    "        self.inputs = tf.placeholder(shape=[None, self.config.max_length], dtype=tf.int32, name='inputs')\n",
    "        self.inputs_length = tf.placeholder(shape=[None], dtype=tf.int32, name='inputs_length')\n",
    "        self.target_ids = tf.placeholder(shape=[None], dtype=tf.int32, name='target_ids')\n",
    "    \n",
    "    def _create_model(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        embedded_inputs = self._embedding(self.inputs)\n",
    "        _, encoder_state = self._encode(embedded_inputs)\n",
    "        # encoder_state = tf.layers.dense(encoder_state, num_units, activation=tf.nn.relu, name='hidden_layer')\n",
    "        self.outputs_logits = tf.layers.dense(encoder_state, self.vocab_size, name='outputs_layer')\n",
    "        self.predicted_id = tf.to_int32(tf.argmax(self.outputs_logits, axis=-1))\n",
    "        \n",
    "    def _create_loss(self):\n",
    "        target_ids_one_hot = tf.one_hot(self.target_ids, self.vocab_size)\n",
    "        target_ids_smoothed = self._label_smoothing(target_ids_one_hot)\n",
    "        cross_ent = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.outputs_logits, labels=target_ids_smoothed)\n",
    "        return tf.reduce_mean(cross_ent)\n",
    "        \n",
    "    def _create_acc(self):\n",
    "        return tf.reduce_mean(tf.to_float(tf.equal(self.target_ids, self.predicted_id)))\n",
    "    \n",
    "    def _create_perplexity(self):\n",
    "        probs = tf.nn.softmax(self.outputs_logits)\n",
    "        print(probs)\n",
    "        target_probs = tf.gather(probs, self.target_ids, axis=1)\n",
    "        print(target_probs)\n",
    "        outputs = tf.reduce_mean(1.0/target_probs)\n",
    "        print(outputs)\n",
    "        return outputs\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        lookup_table = tf.get_variable('lookup_table', shape=[self.vocab_size, self.config.embedding_size], dtype=tf.float32)\n",
    "        embedded_inputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        return embedded_inputs\n",
    "    \n",
    "    def _encode(self, embedded_inputs):\n",
    "        outputs, final_state = self._bidirectional_cell(\n",
    "            embedded_inputs,\n",
    "            self.config.num_layers,\n",
    "            self.config.num_units,\n",
    "            self.config.dropout_in_rate,\n",
    "            self.config.dropout_out_rate\n",
    "        )\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _bidirectional_cell(self, inputs, num_layers, num_units, dropout_in_rate, dropout_out_rate):\n",
    "        cell_fw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_fw')\n",
    "        cell_bw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_bw')\n",
    "        (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            inputs=inputs,\n",
    "            sequence_length=self.inputs_length,\n",
    "            dtype=tf.float32,\n",
    "            scope='bidirectional_cells')\n",
    "        outputs = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        final_state = tf.reduce_sum([fw_state, bw_state], axis=0)\n",
    "        final_state = tf.concat(tf.unstack(final_state, axis=0), axis=-1)\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _gru(self, num_layers: int, num_units: int, dropout_in_rate: float, dropout_out_rate: float, name: str):\n",
    "        cells = []\n",
    "        for l in range(num_layers):\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units, tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(), name=name)\n",
    "            if l == 0:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1-dropout_in_rate)\n",
    "            if l == num_layers-1:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1-dropout_out_rate)\n",
    "            cells.append(cell)\n",
    "        return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    def _label_smoothing(self, inputs, epsilon: float=0.1):\n",
    "        feature_dim = inputs.get_shape().as_list()[-1]\n",
    "        return (1-epsilon) * inputs + (epsilon / feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:25.064560Z",
     "start_time": "2018-09-02T13:24:24.949064Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:25.154566Z",
     "start_time": "2018-09-02T13:24:25.069761Z"
    }
   },
   "outputs": [],
   "source": [
    "def start():\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        now = datetime.now()\n",
    "        logdir = now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "        datasource = PTBDataSource(config)\n",
    "\n",
    "        rnn = RNN(config, datasource.vocab_size)\n",
    "        optimizer = tf.train.AdamOptimizer(config.learning_rate)\n",
    "        train_vars = tf.trainable_variables()\n",
    "        gradients = tf.gradients(rnn.loss, train_vars)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, config.grad_clip)\n",
    "        train_op = optimizer.apply_gradients(zip(clipped_gradients, train_vars), global_step=rnn.global_step)\n",
    "        with tf.name_scope('training'):\n",
    "            s_loss = tf.summary.scalar('loss', rnn.loss)\n",
    "            s_acc = tf.summary.scalar('accuracy', rnn.accuracy)\n",
    "            s_perp = tf.summary.scalar('perplexity', rnn.perplexity)\n",
    "            s_trains = tf.summary.merge([s_loss, s_acc, s_perp])\n",
    "        with tf.name_scope('test'):\n",
    "            test_s_acc = tf.summary.scalar('accuracy', rnn.accuracy)\n",
    "            test_s_perp = tf.summary.scalar('perplexity', rnn.perplexity)\n",
    "        \n",
    "        tf_config = tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=True,\n",
    "            gpu_options=tf.GPUOptions(\n",
    "                allow_growth=True,\n",
    "            ))\n",
    "        with tf.Session(config=tf_config) as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            writer = tf.summary.FileWriter(config.to_log_dir() + '/' + logdir, sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_epoch):\n",
    "                start = time.time()\n",
    "                datasource.shuffle()\n",
    "                batch_list = datasource.feed_dict_list(rnn)\n",
    "                losses = []\n",
    "                accuracies = []\n",
    "                perplexities = []\n",
    "                for (j, fd) in enumerate(batch_list):\n",
    "                    loss, acc, perp, _, smr_train, step = sess.run([rnn.loss, rnn.accuracy, rnn.perplexity, train_op, s_trains, rnn.global_step], feed_dict=fd)\n",
    "                    losses.append(loss)\n",
    "                    accuracies.append(acc)\n",
    "                    perplexities.append(perp)\n",
    "                    writer.add_summary(smr_train, step)\n",
    "                    if j % 100 == 0:\n",
    "                        #print('loss: {:.3f}, acc: {:.3f}'.format(loss, acc))\n",
    "                        inference(sess, rnn, datasource, writer, test_s_acc, test_s_perp, step)\n",
    "                elapsed = time.time() - start\n",
    "                print('epoch {}/{} finished, {} step, elapsed {} sec. loss: {:.3f}, accuracy: {:.3f}, perlexity: {:.3f}'.format(i+1, num_epoch, step, elapsed, np.average(losses), np.average(accuracies), np.average(perplexities)))\n",
    "                # loss が nan なら 飛ばす\n",
    "                if math.isnan(np.average(losses)):\n",
    "                    print('loss is nan')\n",
    "                    break\n",
    "                saver.save(sess, config.to_ckpt_path(), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-02T13:24:25.257897Z",
     "start_time": "2018-09-02T13:24:25.160271Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(sess, model, datasource, writer, s_acc, s_perp, step):\n",
    "    with tf.name_scope('inference'):\n",
    "        test_list = datasource.feed_test_list(model)\n",
    "        acc, smr_acc, smr_perp = sess.run([model.accuracy, s_acc, s_perp], feed_dict=test_list)\n",
    "        writer.add_summary(smr_acc, step)\n",
    "        writer.add_summary(smr_perp, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-02T13:24:23.990Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kentaro.nakanishi/.local/share/virtualenvs/rnn-text-classification-w9qS2Sz3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/kentaro.nakanishi/.local/share/virtualenvs/rnn-text-classification-w9qS2Sz3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Tensor(\"Softmax:0\", shape=(?, 10000), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"GatherV2:0\", shape=(?, ?), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Mean_2:0\", shape=(), dtype=float32, device=/device:GPU:0)\n",
      "epoch 1/400 finished, 327 step, elapsed 36.97825002670288 sec. loss: 7.288, accuracy: 0.070, perlexity: 241713315840.000\n",
      "epoch 2/400 finished, 655 step, elapsed 36.487141847610474 sec. loss: 6.827, accuracy: 0.115, perlexity: 57184.367\n",
      "epoch 3/400 finished, 983 step, elapsed 36.46529030799866 sec. loss: 6.579, accuracy: 0.140, perlexity: 25947.561\n",
      "epoch 4/400 finished, 1311 step, elapsed 36.19972801208496 sec. loss: 6.344, accuracy: 0.159, perlexity: 45814.141\n",
      "epoch 5/400 finished, 1639 step, elapsed 36.34585881233215 sec. loss: 6.259, accuracy: 0.169, perlexity: 66781.391\n",
      "epoch 6/400 finished, 1967 step, elapsed 36.75216031074524 sec. loss: 6.134, accuracy: 0.180, perlexity: 72638.344\n",
      "epoch 7/400 finished, 2295 step, elapsed 35.87955355644226 sec. loss: 6.057, accuracy: 0.189, perlexity: 175337.094\n",
      "epoch 8/400 finished, 2623 step, elapsed 36.209742307662964 sec. loss: 5.970, accuracy: 0.199, perlexity: 615560.438\n",
      "epoch 9/400 finished, 2951 step, elapsed 35.85008478164673 sec. loss: 5.910, accuracy: 0.200, perlexity: 236046.047\n",
      "epoch 10/400 finished, 3279 step, elapsed 36.1633198261261 sec. loss: 5.863, accuracy: 0.206, perlexity: 472917.406\n",
      "epoch 11/400 finished, 3607 step, elapsed 36.5442316532135 sec. loss: 5.802, accuracy: 0.213, perlexity: 234046.922\n",
      "epoch 12/400 finished, 3935 step, elapsed 36.69192981719971 sec. loss: 5.744, accuracy: 0.216, perlexity: 351942.250\n",
      "epoch 13/400 finished, 4263 step, elapsed 36.25204110145569 sec. loss: 5.688, accuracy: 0.221, perlexity: 478076.062\n",
      "epoch 14/400 finished, 4591 step, elapsed 36.647868633270264 sec. loss: 5.650, accuracy: 0.224, perlexity: inf\n",
      "epoch 15/400 finished, 4919 step, elapsed 36.29673361778259 sec. loss: 5.616, accuracy: 0.225, perlexity: 594763.562\n",
      "epoch 16/400 finished, 5247 step, elapsed 36.73911762237549 sec. loss: 5.591, accuracy: 0.228, perlexity: 1181573.500\n",
      "epoch 17/400 finished, 5575 step, elapsed 37.04175591468811 sec. loss: 5.509, accuracy: 0.237, perlexity: 1299739.875\n",
      "epoch 18/400 finished, 5903 step, elapsed 36.41854453086853 sec. loss: 5.548, accuracy: 0.231, perlexity: 1160247.750\n",
      "epoch 19/400 finished, 6231 step, elapsed 36.54111456871033 sec. loss: 5.447, accuracy: 0.241, perlexity: 2420128.250\n",
      "epoch 20/400 finished, 6559 step, elapsed 36.50932598114014 sec. loss: 5.413, accuracy: 0.243, perlexity: 1098703.375\n",
      "epoch 21/400 finished, 6887 step, elapsed 35.9408917427063 sec. loss: 5.432, accuracy: 0.243, perlexity: 1157635.750\n",
      "epoch 22/400 finished, 7215 step, elapsed 36.1758816242218 sec. loss: 5.392, accuracy: 0.244, perlexity: 1391975.375\n",
      "epoch 23/400 finished, 7543 step, elapsed 36.09750819206238 sec. loss: 5.356, accuracy: 0.247, perlexity: 1354021.500\n",
      "epoch 24/400 finished, 7871 step, elapsed 36.67543435096741 sec. loss: 5.319, accuracy: 0.251, perlexity: 5063764.500\n",
      "epoch 25/400 finished, 8199 step, elapsed 36.317394733428955 sec. loss: 5.253, accuracy: 0.259, perlexity: 1921302.250\n",
      "epoch 26/400 finished, 8527 step, elapsed 36.89226150512695 sec. loss: 5.223, accuracy: 0.259, perlexity: 2046423.250\n",
      "epoch 27/400 finished, 8855 step, elapsed 36.08099865913391 sec. loss: 5.217, accuracy: 0.266, perlexity: 1730863.750\n",
      "epoch 28/400 finished, 9183 step, elapsed 36.28312802314758 sec. loss: 5.187, accuracy: 0.266, perlexity: 4206082.500\n",
      "epoch 29/400 finished, 9511 step, elapsed 36.63206243515015 sec. loss: 5.163, accuracy: 0.268, perlexity: 29467424.000\n",
      "epoch 30/400 finished, 9839 step, elapsed 36.03172540664673 sec. loss: 5.144, accuracy: 0.269, perlexity: 2539178.500\n",
      "epoch 31/400 finished, 10167 step, elapsed 36.468220472335815 sec. loss: 5.105, accuracy: 0.271, perlexity: 3653415.000\n",
      "epoch 32/400 finished, 10495 step, elapsed 36.642553091049194 sec. loss: 5.065, accuracy: 0.275, perlexity: 4877966.000\n",
      "epoch 33/400 finished, 10823 step, elapsed 36.41120791435242 sec. loss: 5.086, accuracy: 0.272, perlexity: 18293458.000\n",
      "epoch 34/400 finished, 11151 step, elapsed 36.57013916969299 sec. loss: 5.011, accuracy: 0.280, perlexity: 18516840.000\n",
      "epoch 35/400 finished, 11479 step, elapsed 36.34973359107971 sec. loss: 5.037, accuracy: 0.276, perlexity: 16707770.000\n",
      "epoch 36/400 finished, 11807 step, elapsed 36.71204662322998 sec. loss: 5.025, accuracy: 0.276, perlexity: 28728002.000\n",
      "epoch 37/400 finished, 12135 step, elapsed 36.019054889678955 sec. loss: 4.993, accuracy: 0.282, perlexity: 10880076.000\n",
      "epoch 38/400 finished, 12463 step, elapsed 36.22981381416321 sec. loss: 4.927, accuracy: 0.289, perlexity: 111563512.000\n",
      "epoch 39/400 finished, 12791 step, elapsed 36.732181549072266 sec. loss: 4.919, accuracy: 0.287, perlexity: 13642676.000\n",
      "epoch 40/400 finished, 13119 step, elapsed 36.30537128448486 sec. loss: 4.955, accuracy: 0.284, perlexity: 108608608.000\n",
      "epoch 41/400 finished, 13447 step, elapsed 35.952698945999146 sec. loss: 4.890, accuracy: 0.291, perlexity: 17736922.000\n",
      "epoch 42/400 finished, 13775 step, elapsed 36.37321949005127 sec. loss: 4.862, accuracy: 0.295, perlexity: 86401344.000\n",
      "epoch 43/400 finished, 14103 step, elapsed 35.936683654785156 sec. loss: 4.873, accuracy: 0.295, perlexity: 17354418.000\n",
      "epoch 44/400 finished, 14431 step, elapsed 36.117698669433594 sec. loss: 4.850, accuracy: 0.293, perlexity: 12915822592.000\n",
      "epoch 45/400 finished, 14759 step, elapsed 35.80404210090637 sec. loss: 4.831, accuracy: 0.300, perlexity: 31806526.000\n",
      "epoch 46/400 finished, 15087 step, elapsed 36.535903453826904 sec. loss: 4.806, accuracy: 0.305, perlexity: 61028516.000\n",
      "epoch 47/400 finished, 15415 step, elapsed 36.084651947021484 sec. loss: 4.797, accuracy: 0.299, perlexity: 154408832.000\n",
      "epoch 48/400 finished, 15743 step, elapsed 36.12201142311096 sec. loss: 4.763, accuracy: 0.306, perlexity: 90765144.000\n",
      "epoch 49/400 finished, 16071 step, elapsed 37.07189106941223 sec. loss: 4.772, accuracy: 0.303, perlexity: 21336766.000\n",
      "epoch 50/400 finished, 16399 step, elapsed 36.532819509506226 sec. loss: 4.755, accuracy: 0.302, perlexity: 11495299072.000\n",
      "epoch 51/400 finished, 16727 step, elapsed 36.396262407302856 sec. loss: 4.734, accuracy: 0.309, perlexity: 70995320.000\n",
      "epoch 52/400 finished, 17055 step, elapsed 36.504887342453 sec. loss: 4.685, accuracy: 0.316, perlexity: 35633260.000\n",
      "epoch 53/400 finished, 17383 step, elapsed 36.8140070438385 sec. loss: 4.647, accuracy: 0.319, perlexity: 52352708.000\n",
      "epoch 54/400 finished, 17711 step, elapsed 36.41820740699768 sec. loss: 4.648, accuracy: 0.318, perlexity: 12023386112.000\n",
      "epoch 55/400 finished, 18039 step, elapsed 36.523537397384644 sec. loss: 4.653, accuracy: 0.316, perlexity: 95567800.000\n",
      "epoch 56/400 finished, 18367 step, elapsed 36.697638511657715 sec. loss: 4.609, accuracy: 0.321, perlexity: 486596864.000\n",
      "epoch 57/400 finished, 18695 step, elapsed 36.434266567230225 sec. loss: 4.650, accuracy: 0.319, perlexity: 412941664.000\n",
      "epoch 58/400 finished, 19023 step, elapsed 37.0760715007782 sec. loss: 4.614, accuracy: 0.324, perlexity: 344219456.000\n",
      "epoch 59/400 finished, 19351 step, elapsed 35.94931888580322 sec. loss: 4.617, accuracy: 0.326, perlexity: inf\n",
      "epoch 60/400 finished, 19679 step, elapsed 36.36805820465088 sec. loss: 4.550, accuracy: 0.334, perlexity: 169530720.000\n",
      "epoch 61/400 finished, 20007 step, elapsed 35.767008543014526 sec. loss: 4.538, accuracy: 0.334, perlexity: 852814592.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 62/400 finished, 20335 step, elapsed 35.7199649810791 sec. loss: 4.527, accuracy: 0.336, perlexity: 87273160.000\n",
      "epoch 63/400 finished, 20663 step, elapsed 36.40294170379639 sec. loss: 4.522, accuracy: 0.335, perlexity: 199316352.000\n",
      "epoch 64/400 finished, 20991 step, elapsed 36.331058740615845 sec. loss: 4.512, accuracy: 0.340, perlexity: 288825664.000\n",
      "epoch 65/400 finished, 21319 step, elapsed 36.93815636634827 sec. loss: 4.526, accuracy: 0.337, perlexity: 10384871424.000\n",
      "epoch 66/400 finished, 21647 step, elapsed 36.427062034606934 sec. loss: 4.497, accuracy: 0.339, perlexity: 883379456.000\n",
      "epoch 67/400 finished, 21975 step, elapsed 36.297351121902466 sec. loss: 4.493, accuracy: 0.339, perlexity: 763732992.000\n",
      "epoch 68/400 finished, 22303 step, elapsed 36.648056507110596 sec. loss: 4.406, accuracy: 0.353, perlexity: 1750180608.000\n",
      "epoch 69/400 finished, 22631 step, elapsed 36.06713938713074 sec. loss: 4.393, accuracy: 0.357, perlexity: 2678583296.000\n",
      "epoch 70/400 finished, 22959 step, elapsed 36.71292209625244 sec. loss: 4.433, accuracy: 0.348, perlexity: 1187835648.000\n",
      "epoch 71/400 finished, 23287 step, elapsed 35.97199273109436 sec. loss: 4.406, accuracy: 0.355, perlexity: 596861824.000\n",
      "epoch 72/400 finished, 23615 step, elapsed 36.36670970916748 sec. loss: 4.356, accuracy: 0.359, perlexity: 2328729344.000\n",
      "epoch 73/400 finished, 23943 step, elapsed 36.34256935119629 sec. loss: 4.314, accuracy: 0.366, perlexity: 28821910716416.000\n",
      "epoch 74/400 finished, 24271 step, elapsed 36.48762583732605 sec. loss: 4.386, accuracy: 0.355, perlexity: 13452886016.000\n",
      "epoch 75/400 finished, 24599 step, elapsed 36.61548137664795 sec. loss: 4.321, accuracy: 0.364, perlexity: 224884277248.000\n",
      "epoch 76/400 finished, 24927 step, elapsed 36.234036684036255 sec. loss: 4.360, accuracy: 0.357, perlexity: 790005632.000\n",
      "epoch 77/400 finished, 25255 step, elapsed 36.488282203674316 sec. loss: 4.333, accuracy: 0.365, perlexity: 579130368.000\n",
      "epoch 78/400 finished, 25583 step, elapsed 36.1571364402771 sec. loss: 4.321, accuracy: 0.367, perlexity: 9531106881699840.000\n",
      "epoch 79/400 finished, 25911 step, elapsed 35.64267873764038 sec. loss: 4.338, accuracy: 0.365, perlexity: 29456199680.000\n",
      "epoch 80/400 finished, 26239 step, elapsed 36.35412120819092 sec. loss: 4.282, accuracy: 0.370, perlexity: 454757760.000\n",
      "epoch 81/400 finished, 26567 step, elapsed 36.96148991584778 sec. loss: 4.244, accuracy: 0.377, perlexity: 175535636480.000\n",
      "epoch 82/400 finished, 26895 step, elapsed 36.60938811302185 sec. loss: 4.280, accuracy: 0.370, perlexity: 6459828736.000\n",
      "epoch 83/400 finished, 27223 step, elapsed 35.94681143760681 sec. loss: 4.268, accuracy: 0.368, perlexity: 128493797376.000\n",
      "epoch 84/400 finished, 27551 step, elapsed 36.83690786361694 sec. loss: 4.172, accuracy: 0.390, perlexity: 35136864256.000\n",
      "epoch 85/400 finished, 27879 step, elapsed 36.95731854438782 sec. loss: 4.251, accuracy: 0.377, perlexity: 414527651840.000\n",
      "epoch 86/400 finished, 28207 step, elapsed 36.18263077735901 sec. loss: 4.182, accuracy: 0.386, perlexity: 874795712.000\n",
      "epoch 87/400 finished, 28535 step, elapsed 36.466073989868164 sec. loss: 4.184, accuracy: 0.387, perlexity: 22783580160.000\n",
      "epoch 88/400 finished, 28863 step, elapsed 36.09849762916565 sec. loss: 4.169, accuracy: 0.389, perlexity: 3519618048.000\n",
      "epoch 89/400 finished, 29191 step, elapsed 36.55940556526184 sec. loss: 4.167, accuracy: 0.385, perlexity: 1323441152.000\n",
      "epoch 90/400 finished, 29519 step, elapsed 36.6089551448822 sec. loss: 4.133, accuracy: 0.397, perlexity: 3357581824.000\n",
      "epoch 91/400 finished, 29847 step, elapsed 36.86271858215332 sec. loss: 4.137, accuracy: 0.391, perlexity: 6338570752.000\n",
      "epoch 92/400 finished, 30175 step, elapsed 36.1724534034729 sec. loss: 4.155, accuracy: 0.390, perlexity: 2491464448.000\n",
      "epoch 93/400 finished, 30503 step, elapsed 36.407042026519775 sec. loss: 4.118, accuracy: 0.398, perlexity: 13115114496.000\n",
      "epoch 94/400 finished, 30831 step, elapsed 36.27287745475769 sec. loss: 4.112, accuracy: 0.397, perlexity: 4903691776.000\n",
      "epoch 95/400 finished, 31159 step, elapsed 36.535863399505615 sec. loss: 4.138, accuracy: 0.390, perlexity: 23600572416.000\n",
      "epoch 96/400 finished, 31487 step, elapsed 36.094350814819336 sec. loss: 4.108, accuracy: 0.399, perlexity: 130879012864.000\n",
      "epoch 97/400 finished, 31815 step, elapsed 36.072784423828125 sec. loss: 4.139, accuracy: 0.392, perlexity: 13771022336.000\n",
      "epoch 98/400 finished, 32143 step, elapsed 36.6901741027832 sec. loss: 4.055, accuracy: 0.409, perlexity: 59364581376.000\n",
      "epoch 99/400 finished, 32471 step, elapsed 36.61174821853638 sec. loss: 4.103, accuracy: 0.400, perlexity: 80689250304.000\n",
      "epoch 100/400 finished, 32799 step, elapsed 36.23211598396301 sec. loss: 4.080, accuracy: 0.401, perlexity: 2995295744.000\n",
      "epoch 101/400 finished, 33127 step, elapsed 37.012428522109985 sec. loss: 4.072, accuracy: 0.404, perlexity: 80829363519488.000\n",
      "epoch 102/400 finished, 33455 step, elapsed 36.14697885513306 sec. loss: 4.023, accuracy: 0.410, perlexity: 86779961344.000\n",
      "epoch 103/400 finished, 33783 step, elapsed 36.47210764884949 sec. loss: 3.950, accuracy: 0.425, perlexity: 22672402432.000\n",
      "epoch 104/400 finished, 34111 step, elapsed 36.50046682357788 sec. loss: 3.999, accuracy: 0.416, perlexity: 17804412928.000\n",
      "epoch 105/400 finished, 34439 step, elapsed 36.470629930496216 sec. loss: 4.015, accuracy: 0.415, perlexity: 52560195584.000\n",
      "epoch 106/400 finished, 34767 step, elapsed 36.47693395614624 sec. loss: 3.986, accuracy: 0.421, perlexity: 3219430912.000\n",
      "epoch 107/400 finished, 35095 step, elapsed 36.80915665626526 sec. loss: 4.032, accuracy: 0.410, perlexity: 40905814016.000\n",
      "epoch 108/400 finished, 35423 step, elapsed 36.257328748703 sec. loss: 3.985, accuracy: 0.423, perlexity: 5171795968.000\n",
      "epoch 109/400 finished, 35751 step, elapsed 36.77226519584656 sec. loss: 4.003, accuracy: 0.415, perlexity: 5453615616.000\n",
      "epoch 110/400 finished, 36079 step, elapsed 36.69706439971924 sec. loss: 4.004, accuracy: 0.417, perlexity: 184623677440.000\n",
      "epoch 111/400 finished, 36407 step, elapsed 36.32423114776611 sec. loss: 3.959, accuracy: 0.423, perlexity: 42911125504.000\n",
      "epoch 112/400 finished, 36735 step, elapsed 36.40286993980408 sec. loss: 3.940, accuracy: 0.428, perlexity: 11042295906304.000\n",
      "epoch 113/400 finished, 37063 step, elapsed 36.523441791534424 sec. loss: 3.976, accuracy: 0.422, perlexity: 21345565933568.000\n",
      "epoch 114/400 finished, 37391 step, elapsed 35.85262393951416 sec. loss: 3.913, accuracy: 0.432, perlexity: 24139038720.000\n",
      "epoch 115/400 finished, 37719 step, elapsed 36.303795337677 sec. loss: 3.934, accuracy: 0.427, perlexity: 36769738752.000\n",
      "epoch 116/400 finished, 38047 step, elapsed 35.994181394577026 sec. loss: 3.932, accuracy: 0.427, perlexity: 19663002009600.000\n",
      "epoch 117/400 finished, 38375 step, elapsed 35.790518045425415 sec. loss: 3.884, accuracy: 0.437, perlexity: 259575169024.000\n",
      "epoch 118/400 finished, 38703 step, elapsed 36.49023127555847 sec. loss: 3.876, accuracy: 0.439, perlexity: 41303920345088.000\n",
      "epoch 119/400 finished, 39031 step, elapsed 36.76046013832092 sec. loss: 3.918, accuracy: 0.431, perlexity: 34032390144.000\n",
      "epoch 120/400 finished, 39359 step, elapsed 36.634838581085205 sec. loss: 3.929, accuracy: 0.430, perlexity: 242789744640.000\n",
      "epoch 121/400 finished, 39687 step, elapsed 36.45746850967407 sec. loss: 3.873, accuracy: 0.439, perlexity: 254686265344.000\n",
      "epoch 122/400 finished, 40015 step, elapsed 35.786272048950195 sec. loss: 3.826, accuracy: 0.448, perlexity: 373312552960.000\n",
      "epoch 123/400 finished, 40343 step, elapsed 36.53226017951965 sec. loss: 3.839, accuracy: 0.447, perlexity: 11548147712.000\n",
      "epoch 124/400 finished, 40671 step, elapsed 36.033730030059814 sec. loss: 3.879, accuracy: 0.437, perlexity: 381158096896.000\n",
      "epoch 125/400 finished, 40999 step, elapsed 36.09381437301636 sec. loss: 3.792, accuracy: 0.451, perlexity: 261775572992.000\n",
      "epoch 126/400 finished, 41327 step, elapsed 36.40517568588257 sec. loss: 3.848, accuracy: 0.442, perlexity: 44223078400.000\n",
      "epoch 127/400 finished, 41655 step, elapsed 36.351741790771484 sec. loss: 3.838, accuracy: 0.440, perlexity: 66648399872.000\n",
      "epoch 128/400 finished, 41983 step, elapsed 37.00846076011658 sec. loss: 3.777, accuracy: 0.454, perlexity: 743865909248.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 129/400 finished, 42311 step, elapsed 36.11920094490051 sec. loss: 3.798, accuracy: 0.450, perlexity: 245958508544.000\n",
      "epoch 130/400 finished, 42639 step, elapsed 36.045727252960205 sec. loss: 3.810, accuracy: 0.453, perlexity: 1998436433920.000\n",
      "epoch 131/400 finished, 42967 step, elapsed 37.263872146606445 sec. loss: 3.832, accuracy: 0.438, perlexity: 39421882368.000\n",
      "epoch 132/400 finished, 43295 step, elapsed 36.24377703666687 sec. loss: 3.724, accuracy: 0.466, perlexity: 277750743040.000\n",
      "epoch 133/400 finished, 43623 step, elapsed 36.22351312637329 sec. loss: 3.797, accuracy: 0.453, perlexity: 34968281088.000\n",
      "epoch 134/400 finished, 43951 step, elapsed 36.774821043014526 sec. loss: 3.756, accuracy: 0.461, perlexity: inf\n",
      "epoch 135/400 finished, 44279 step, elapsed 36.12817740440369 sec. loss: 3.747, accuracy: 0.461, perlexity: 1371308032000.000\n",
      "epoch 136/400 finished, 44607 step, elapsed 36.07532334327698 sec. loss: 3.739, accuracy: 0.463, perlexity: inf\n",
      "epoch 137/400 finished, 44935 step, elapsed 36.47866106033325 sec. loss: 3.721, accuracy: 0.465, perlexity: 723891322880.000\n",
      "epoch 138/400 finished, 45263 step, elapsed 36.00631332397461 sec. loss: 3.684, accuracy: 0.474, perlexity: 552625111040.000\n",
      "epoch 139/400 finished, 45591 step, elapsed 36.43225145339966 sec. loss: 3.695, accuracy: 0.471, perlexity: 1311817465856.000\n",
      "epoch 140/400 finished, 45919 step, elapsed 36.300036907196045 sec. loss: 3.733, accuracy: 0.460, perlexity: 16269505986560.000\n",
      "epoch 141/400 finished, 46247 step, elapsed 36.03113675117493 sec. loss: 3.695, accuracy: 0.468, perlexity: 269586825216.000\n",
      "epoch 142/400 finished, 46575 step, elapsed 36.310138463974 sec. loss: 3.726, accuracy: 0.462, perlexity: 2492281913344.000\n",
      "epoch 143/400 finished, 46903 step, elapsed 36.09273886680603 sec. loss: 3.685, accuracy: 0.469, perlexity: 13333166181574508544.000\n",
      "epoch 144/400 finished, 47231 step, elapsed 36.445683002471924 sec. loss: 3.685, accuracy: 0.474, perlexity: 6174578049024.000\n",
      "epoch 145/400 finished, 47559 step, elapsed 36.50675868988037 sec. loss: 3.704, accuracy: 0.470, perlexity: inf\n",
      "epoch 146/400 finished, 47887 step, elapsed 36.46856331825256 sec. loss: 3.709, accuracy: 0.468, perlexity: 497313316864.000\n",
      "epoch 147/400 finished, 48215 step, elapsed 35.79791188240051 sec. loss: 3.664, accuracy: 0.475, perlexity: 186590937088.000\n",
      "epoch 148/400 finished, 48543 step, elapsed 36.274354219436646 sec. loss: 3.665, accuracy: 0.477, perlexity: 645606735872.000\n",
      "epoch 149/400 finished, 48871 step, elapsed 36.219199419021606 sec. loss: 3.656, accuracy: 0.479, perlexity: 32625305583616.000\n",
      "epoch 150/400 finished, 49199 step, elapsed 35.98519015312195 sec. loss: 3.676, accuracy: 0.475, perlexity: 1472952664064.000\n",
      "epoch 151/400 finished, 49527 step, elapsed 36.365161180496216 sec. loss: 3.634, accuracy: 0.482, perlexity: 6266088849408.000\n",
      "epoch 152/400 finished, 49855 step, elapsed 36.451430320739746 sec. loss: 3.642, accuracy: 0.479, perlexity: 1679654518784.000\n",
      "epoch 153/400 finished, 50183 step, elapsed 36.794508934020996 sec. loss: 3.616, accuracy: 0.487, perlexity: 135062855680.000\n",
      "epoch 154/400 finished, 50511 step, elapsed 36.35685157775879 sec. loss: 3.645, accuracy: 0.479, perlexity: 29487909719506944.000\n",
      "epoch 155/400 finished, 50839 step, elapsed 36.80074119567871 sec. loss: 3.595, accuracy: 0.491, perlexity: 119342063616.000\n",
      "epoch 156/400 finished, 51167 step, elapsed 36.206753730773926 sec. loss: 3.609, accuracy: 0.488, perlexity: 1244539387904.000\n",
      "epoch 157/400 finished, 51495 step, elapsed 36.295477867126465 sec. loss: 3.595, accuracy: 0.486, perlexity: 8662126428160.000\n",
      "epoch 158/400 finished, 51823 step, elapsed 36.290979862213135 sec. loss: 3.636, accuracy: 0.479, perlexity: 378816593920.000\n",
      "epoch 159/400 finished, 52151 step, elapsed 36.720149517059326 sec. loss: 3.561, accuracy: 0.494, perlexity: 3771232157696.000\n",
      "epoch 160/400 finished, 52479 step, elapsed 36.55597257614136 sec. loss: 3.652, accuracy: 0.476, perlexity: 381904264101888.000\n",
      "epoch 161/400 finished, 52807 step, elapsed 36.213058948516846 sec. loss: 3.593, accuracy: 0.488, perlexity: 11368574421565440.000\n",
      "epoch 162/400 finished, 53135 step, elapsed 36.26811385154724 sec. loss: 3.596, accuracy: 0.488, perlexity: 14600031436800.000\n",
      "epoch 163/400 finished, 53463 step, elapsed 36.31376838684082 sec. loss: 3.574, accuracy: 0.491, perlexity: 522559389696.000\n",
      "epoch 164/400 finished, 53791 step, elapsed 36.656150579452515 sec. loss: 3.517, accuracy: 0.501, perlexity: 1212924936847360.000\n",
      "epoch 165/400 finished, 54119 step, elapsed 36.22260785102844 sec. loss: 3.550, accuracy: 0.496, perlexity: 12414454071296.000\n",
      "epoch 166/400 finished, 54447 step, elapsed 36.725685358047485 sec. loss: 3.529, accuracy: 0.500, perlexity: inf\n",
      "epoch 167/400 finished, 54775 step, elapsed 36.29760217666626 sec. loss: 3.550, accuracy: 0.498, perlexity: 496789258240.000\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    with tf.Graph().as_default():\n",
    "        start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T08:18:31.466637Z",
     "start_time": "2018-08-05T08:18:27.913665Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
