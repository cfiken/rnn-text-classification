{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:43.515894Z",
     "start_time": "2018-07-30T14:39:42.878693Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:43.518847Z",
     "start_time": "2018-07-30T14:39:43.517069Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import NamedTuple, List, Dict, Tuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:32:44.683063Z",
     "start_time": "2018-07-30T14:32:44.606838Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:43.580731Z",
     "start_time": "2018-07-30T14:39:43.519893Z"
    }
   },
   "outputs": [],
   "source": [
    "class orthogonal_initializer_without_nan(tf.orthogonal_initializer):\n",
    "    '''\n",
    "    tf1.5 の orthogonal_initializer は NaN を出す可能性があります。\n",
    "    see: https://vantage-planning.qiita.com/halhorn/items/d0640c9eebf5ee5842e9\n",
    "    効率を犠牲にこれを回避するための initializer です。\n",
    "\n",
    "    sample_num 個初期値をサンプリングして、そのうち NaN を含まないものを返します。\n",
    "    NaN を生成してしまった場合には例外を発します。その場合、コンストラクタの sample_num を大きくしてください。\n",
    "\n",
    "    orthogonal_initializer については以下にコードがあります。\n",
    "    https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/init_ops.py#L479\n",
    "    '''\n",
    "    def __init__(self, gain=1.0, seed=None, dtype=tf.float32, sample_num=2):\n",
    "        super(orthogonal_initializer_without_nan, self).__init__(gain, seed, dtype)\n",
    "        self._sample_num = sample_num\n",
    "\n",
    "    def __call__(self, shape, dtype=None, partition_info=None):\n",
    "        samples = [\n",
    "            super(orthogonal_initializer_without_nan, self).__call__(shape, dtype, partition_info)\n",
    "            for i in range(self._sample_num)\n",
    "        ]\n",
    "        no_nan_val = tf.case([(tf.reduce_any(tf.is_nan(t)), lambda: t) for t in samples])\n",
    "        name = 'orthogonal_initializer_without_nan'\n",
    "        return tf.check_numerics(no_nan_val, name=name, message=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:43.670552Z",
     "start_time": "2018-07-30T14:39:43.582581Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(NamedTuple):\n",
    "    num_units: int = 256\n",
    "    num_layers: int = 6\n",
    "    embedding_size = 256\n",
    "    batch_size: int = 64\n",
    "    max_length: int = 50\n",
    "    dropout_in_rate: float = 0.1\n",
    "    dropout_out_rate: float = 0.2\n",
    "    learning_rate: float = 0.0001\n",
    "    checkpoint_dir = './checkpoints/'\n",
    "    data_path: str = './data/'\n",
    "    log_dir = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:43.772256Z",
     "start_time": "2018-07-30T14:39:43.672749Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:43.900068Z",
     "start_time": "2018-07-30T14:39:43.777893Z"
    }
   },
   "outputs": [],
   "source": [
    "class PTBDataSource:\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        train_path = os.path.join(self.config.data_path, 'ptb.train.txt')\n",
    "        test_path = os.path.join(self.config.data_path, 'ptb.test.txt')\n",
    "        valid_path = os.path.join(self.config.data_path, 'ptb.valid.txt')\n",
    "        \n",
    "        self._word_to_id = self._create_tokenizer(train_path)\n",
    "        self._id_to_word = {v: k for k, v in self._word_to_id.items()}\n",
    "        \n",
    "        self.train = self._create_data(train_path)\n",
    "        self.test = self._create_data(test_path)\n",
    "        self.valid = self._create_data(valid_path)\n",
    "        \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.train)\n",
    "        \n",
    "    def feed_dict_list(self, model):\n",
    "        num_batch = len(self.train) // self.config.batch_size\n",
    "        data_list = []\n",
    "        batch_list = []\n",
    "        inputs = []\n",
    "        inputs_length = []\n",
    "        target_ids = []\n",
    "        \n",
    "        # まず全部feedの形にする\n",
    "        for (i, sentence) in enumerate(self.train):\n",
    "            for j in range(len(sentence)-1):\n",
    "                inputs_words = sentence[:j+1][-self.config.max_length:] \n",
    "                inputs.append(inputs_words + [0] * (self.config.max_length - len(inputs_words)))\n",
    "                inputs_length.append(len(inputs_words))\n",
    "                target_ids.append(sentence[j+1])\n",
    "        inputs = np.array(inputs)\n",
    "        inputs_length = np.array(inputs_length)\n",
    "        target_ids = np.array(target_ids)\n",
    "        \n",
    "        # batch_sizeに分ける\n",
    "        for i in range(num_batch):\n",
    "            index_from = i * self.config.batch_size\n",
    "            index_to = (i + 1) * self.config.batch_size\n",
    "            batch_range = range(index_from, index_to)\n",
    "            fd = {\n",
    "                model.inputs: inputs[batch_range],\n",
    "                model.inputs_length: inputs_length[batch_range],\n",
    "                model.target_ids: target_ids[batch_range]\n",
    "            }\n",
    "            batch_list.append(fd)\n",
    "        return batch_list\n",
    "            \n",
    "    def _read_all_words(self, path) -> List[str]:\n",
    "        with open(path, 'r') as f:\n",
    "            return f.read().replace('\\n', '<eos>').split()\n",
    "        \n",
    "    def _read_sentences(self, path) -> List[List[str]]:\n",
    "        with open(path, 'r') as f:\n",
    "            sentences = f.read().split('\\n')\n",
    "            return [sentence.split() for sentence in sentences]\n",
    "\n",
    "    def _create_tokenizer(self, path: str):\n",
    "        data = self._read_all_words(path)\n",
    "        counter = Counter(data)\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "        words, _ = list(zip(*sorted_counter))\n",
    "        word_to_id = dict(zip(words, range(1, len(words)+1)))\n",
    "        return word_to_id\n",
    "        \n",
    "    def _get_id_from_word(self, word: str) -> int:\n",
    "        return self._word_to_id.get(word, self.unk_id)\n",
    "    \n",
    "    def _sentence_to_id_list(self, sentence: List[str]) -> List[int]:\n",
    "        return [self._get_id_from_word(word) for word in sentence]\n",
    "    \n",
    "    def _get_word_from_id(self, word_id: int) -> str:\n",
    "        return self._id_to_word.get(word_id, self.unk_str)\n",
    "    \n",
    "    def _create_data(self, path: str):\n",
    "        return [self._sentence_to_id_list(sentence) for sentence in self._read_sentences(path)]\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def unk_id(self) -> int:\n",
    "        return self._word_to_id.get('<unk>', self.pad_id)\n",
    "    \n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._word_to_id.get('<eos>', self.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:44.849841Z",
     "start_time": "2018-07-30T14:39:43.901622Z"
    }
   },
   "outputs": [],
   "source": [
    "datasource = PTBDataSource(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:44.860623Z",
     "start_time": "2018-07-30T14:39:44.851092Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, config: Config, vocab_size):\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "        self._create_placeholder()\n",
    "        self._create_model()\n",
    "        self.loss = self._create_loss()\n",
    "        self.accuracy = self._create_acc()\n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        self.is_training = tf.placeholder(shape=(), dtype=tf.bool, name='is_training')\n",
    "        self.inputs = tf.placeholder(shape=[None, self.config.max_length], dtype=tf.int32, name='inputs')\n",
    "        self.inputs_length = tf.placeholder(shape=[None], dtype=tf.int32, name='inputs_length')\n",
    "        self.target_ids = tf.placeholder(shape=[None], dtype=tf.int32, name='target_ids')\n",
    "    \n",
    "    def _create_model(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        embedded_inputs = self._embedding(self.inputs)\n",
    "        _, encoder_state = self._encode(embedded_inputs)\n",
    "        # encoder_state = tf.layers.dense(encoder_state, num_units, activation=tf.nn.relu, name='hidden_layer')\n",
    "        self.outputs_logits = tf.layers.dense(encoder_state, self.vocab_size, activation=tf.nn.softmax, name='outputs_layer')\n",
    "        self.predicted_id = tf.to_int32(tf.argmax(self.outputs_logits, axis=-1))\n",
    "        \n",
    "    def _create_loss(self):\n",
    "        is_target = tf.to_float(tf.not_equal(self.target_ids, 0))\n",
    "        target_ids_one_hot = tf.one_hot(self.target_ids, self.vocab_size)\n",
    "        target_ids_smoothed = self._label_smoothing(target_ids_one_hot)\n",
    "        cross_ent = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.outputs_logits, labels=target_ids_smoothed)\n",
    "        return tf.reduce_sum(cross_ent * is_target) / tf.reduce_sum(is_target)\n",
    "        \n",
    "    def _create_acc(self):\n",
    "        return tf.reduce_mean(tf.to_float(tf.equal(self.target_ids, self.predicted_id)))\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        lookup_table = tf.get_variable('lookup_table', shape=[self.vocab_size, self.config.embedding_size], dtype=tf.float32)\n",
    "        embedded_inputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        return embedded_inputs\n",
    "    \n",
    "    def _encode(self, embedded_inputs):\n",
    "        outputs, final_state = self._bidirectional_cell(\n",
    "            embedded_inputs,\n",
    "            self.config.num_layers,\n",
    "            self.config.num_units,\n",
    "            self.config.dropout_in_rate,\n",
    "            self.config.dropout_out_rate\n",
    "        )\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _bidirectional_cell(self, inputs, num_layers, num_units, dropout_in_rate, dropout_out_rate):\n",
    "        cell_fw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_fw')\n",
    "        cell_bw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_bw')\n",
    "        (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            inputs=inputs,\n",
    "            sequence_length=self.inputs_length,\n",
    "            dtype=tf.float32,\n",
    "            scope='bidirectional_cells')\n",
    "        outputs = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        final_state = tf.reduce_sum([fw_state, bw_state], axis=0)\n",
    "        final_state = tf.concat(tf.unstack(final_state, axis=0), axis=-1)\n",
    "        print('final_state: ', final_state.shape)\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _gru(self, num_layers: int, num_units: int, dropout_in_rate: float, dropout_out_rate: float, name: str):\n",
    "        cells = []\n",
    "        for l in range(num_layers):\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units, tf.nn.relu, kernel_initializer=orthogonal_initializer_without_nan, name=name)\n",
    "            if l == 0:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1-dropout_in_rate)\n",
    "            if l == num_layers-1:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1-dropout_out_rate)\n",
    "            cells.append(cell)\n",
    "        return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    def _label_smoothing(self, inputs, epsilon: float=0.1):\n",
    "        feature_dim = inputs.get_shape().as_list()[-1]\n",
    "        return (1-epsilon) * inputs + (epsilon / feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:46.611277Z",
     "start_time": "2018-07-30T14:39:44.861795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kentaro.nakanishi/.local/share/virtualenvs/rnn-text-classification-w9qS2Sz3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/kentaro.nakanishi/.local/share/virtualenvs/rnn-text-classification-w9qS2Sz3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "final_state:  (?, 1536)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(config, datasource.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:48.854643Z",
     "start_time": "2018-07-30T14:39:46.612318Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(config.learning_rate)\n",
    "train_op = optimizer.minimize(rnn.loss, global_step=rnn.global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:48.859446Z",
     "start_time": "2018-07-30T14:39:48.855778Z"
    }
   },
   "outputs": [],
   "source": [
    "s_loss = tf.summary.scalar('loss', rnn.loss)\n",
    "s_acc = tf.summary.scalar('accuracy', rnn.accuracy)\n",
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:48.953281Z",
     "start_time": "2018-07-30T14:39:48.860400Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-30T14:39:49.025216Z",
     "start_time": "2018-07-30T14:39:48.957956Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "logdir = now.strftime(\"%Y%m%d-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-30T14:39:42.888Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.210, acc: 0.000\n",
      "loss: 9.210, acc: 0.047\n",
      "loss: 9.209, acc: 0.094\n",
      "loss: 9.158, acc: 0.062\n",
      "loss: 9.182, acc: 0.062\n",
      "loss: 9.210, acc: 0.016\n",
      "loss: 9.182, acc: 0.031\n",
      "epoch 0/100 finished, 656 step. average loss: 9.183, average accuracy: 0.050\n",
      "loss: 9.182, acc: 0.031\n",
      "loss: 9.196, acc: 0.016\n",
      "loss: 9.140, acc: 0.078\n",
      "loss: 9.183, acc: 0.031\n",
      "loss: 9.168, acc: 0.078\n",
      "loss: 9.098, acc: 0.125\n",
      "loss: 9.140, acc: 0.094\n",
      "epoch 1/100 finished, 1313 step. average loss: 9.166, average accuracy: 0.053\n",
      "loss: 9.154, acc: 0.062\n",
      "loss: 9.155, acc: 0.062\n",
      "loss: 9.140, acc: 0.078\n",
      "loss: 9.154, acc: 0.062\n",
      "loss: 9.168, acc: 0.047\n",
      "loss: 9.168, acc: 0.047\n",
      "loss: 9.182, acc: 0.031\n",
      "epoch 2/100 finished, 1970 step. average loss: 9.165, average accuracy: 0.052\n",
      "loss: 9.182, acc: 0.031\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    writer = tf.summary.FileWriter(config.log_dir + logdir, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_epoch):\n",
    "        datasource.shuffle()\n",
    "        batch_list = datasource.feed_dict_list(rnn)\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        for (j, fd) in enumerate(batch_list):\n",
    "            loss, acc, _, smr, step = sess.run([rnn.loss, rnn.accuracy, train_op, merged_summary, rnn.global_step], feed_dict=fd)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(acc)\n",
    "            writer.add_summary(smr, step)\n",
    "            if j % 100 == 0:\n",
    "                print('loss: {:.3f}, acc: {:.3f}'.format(loss, acc))\n",
    "        print('epoch {}/{} finished, {} step. average loss: {:.3f}, average accuracy: {:.3f}'.format(i, num_epoch, step, np.average(losses), np.average(accuracies)))\n",
    "        saver.save(sess, config.checkpoint_dir + \"model.ckpt\", global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
