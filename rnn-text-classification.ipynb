{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:19.630135Z",
     "start_time": "2018-08-01T16:27:19.007742Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:19.633134Z",
     "start_time": "2018-08-01T16:27:19.631297Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import NamedTuple, List, Dict, Tuple\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:19.745937Z",
     "start_time": "2018-08-01T16:27:19.634258Z"
    }
   },
   "outputs": [],
   "source": [
    "class orthogonal_initializer_without_nan(tf.orthogonal_initializer):\n",
    "    '''\n",
    "    tf1.5 の orthogonal_initializer は NaN を出す可能性があります。\n",
    "    see: https://vantage-planning.qiita.com/halhorn/items/d0640c9eebf5ee5842e9\n",
    "    効率を犠牲にこれを回避するための initializer です。\n",
    "\n",
    "    sample_num 個初期値をサンプリングして、そのうち NaN を含まないものを返します。\n",
    "    NaN を生成してしまった場合には例外を発します。その場合、コンストラクタの sample_num を大きくしてください。\n",
    "\n",
    "    orthogonal_initializer については以下にコードがあります。\n",
    "    https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/ops/init_ops.py#L479\n",
    "    '''\n",
    "    def __init__(self, gain=1.0, seed=None, dtype=tf.float32, sample_num=2):\n",
    "        super(orthogonal_initializer_without_nan, self).__init__(gain, seed, dtype)\n",
    "        self._sample_num = sample_num\n",
    "\n",
    "    def __call__(self, shape, dtype=None, partition_info=None):\n",
    "        samples = [\n",
    "            super(orthogonal_initializer_without_nan, self).__call__(shape, dtype, partition_info)\n",
    "            for i in range(self._sample_num)\n",
    "        ]\n",
    "        no_nan_val = tf.case([(tf.reduce_any(tf.is_nan(t)), lambda: t) for t in samples])\n",
    "        name = 'orthogonal_initializer_without_nan'\n",
    "        return tf.check_numerics(no_nan_val, name=name, message=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:19.856447Z",
     "start_time": "2018-08-01T16:27:19.752560Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config(NamedTuple):\n",
    "    num_units: int = 512\n",
    "    num_layers: int = 6\n",
    "    embedding_size = 512\n",
    "    batch_size: int = 128\n",
    "    max_length: int = 50\n",
    "    dropout_in_rate: float = 0.1\n",
    "    dropout_out_rate: float = 0.2\n",
    "    learning_rate: float = 0.001\n",
    "    checkpoint_dir = './checkpoints/'\n",
    "    data_path: str = './data/'\n",
    "    log_dir = './logs/'\n",
    "    \n",
    "    def to_log_dir(self) -> str:\n",
    "        return self.log_dir + 'layers={}/units={}/lr={}'.format(self.num_layers, self.num_units, self.learning_rate)\n",
    "    \n",
    "    def to_ckpt_path(self) -> str:\n",
    "        return self.checkpoint_dir + 'l{}_u{}_lr{}_model.ckpt'.format(self.num_layers, self.num_units, self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:19.990388Z",
     "start_time": "2018-08-01T16:27:19.861867Z"
    }
   },
   "outputs": [],
   "source": [
    "units = [128, 256, 512, 1024]\n",
    "layers = [4, 6, 8]\n",
    "lrs = [0.01, 0.001, 0.0001]\n",
    "configs = []\n",
    "for l in layers:\n",
    "    for u in units:\n",
    "        for lr in lrs:\n",
    "            configs.append(Config(num_layers=l, num_units=u, learning_rate=lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:20.136284Z",
     "start_time": "2018-08-01T16:27:19.995317Z"
    }
   },
   "outputs": [],
   "source": [
    "class PTBDataSource:\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        train_path = os.path.join(self.config.data_path, 'ptb.train.txt')\n",
    "        test_path = os.path.join(self.config.data_path, 'ptb.test.txt')\n",
    "        valid_path = os.path.join(self.config.data_path, 'ptb.valid.txt')\n",
    "        \n",
    "        self._word_to_id = self._create_tokenizer(train_path)\n",
    "        self._id_to_word = {v: k for k, v in self._word_to_id.items()}\n",
    "        \n",
    "        self.train = self._create_data(train_path)\n",
    "        self.test = self._create_data(test_path)\n",
    "        self.valid = self._create_data(valid_path)\n",
    "        \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.train)\n",
    "        \n",
    "    def feed_dict_list(self, model):\n",
    "        num_batch = len(self.train) // self.config.batch_size\n",
    "        data_list = []\n",
    "        batch_list = []\n",
    "        inputs = []\n",
    "        inputs_length = []\n",
    "        target_ids = []\n",
    "        \n",
    "        # まず全部feedの形にする\n",
    "        for (i, sentence) in enumerate(self.train):\n",
    "            for j in range(len(sentence)-1):\n",
    "                inputs_words = sentence[:j+1][-self.config.max_length:] \n",
    "                inputs.append(inputs_words + [0] * (self.config.max_length - len(inputs_words)))\n",
    "                inputs_length.append(len(inputs_words))\n",
    "                target_ids.append(sentence[j+1])\n",
    "        inputs = np.array(inputs)\n",
    "        inputs_length = np.array(inputs_length)\n",
    "        target_ids = np.array(target_ids)\n",
    "        \n",
    "        # batch_sizeに分ける\n",
    "        for i in range(num_batch):\n",
    "            index_from = i * self.config.batch_size\n",
    "            index_to = (i + 1) * self.config.batch_size\n",
    "            batch_range = range(index_from, index_to)\n",
    "            fd = {\n",
    "                model.inputs: inputs[batch_range],\n",
    "                model.inputs_length: inputs_length[batch_range],\n",
    "                model.target_ids: target_ids[batch_range]\n",
    "            }\n",
    "            batch_list.append(fd)\n",
    "        return batch_list\n",
    "            \n",
    "    def _read_all_words(self, path) -> List[str]:\n",
    "        with open(path, 'r') as f:\n",
    "            return f.read().replace('\\n', '<eos>').split()\n",
    "        \n",
    "    def _read_sentences(self, path) -> List[List[str]]:\n",
    "        with open(path, 'r') as f:\n",
    "            sentences = f.read().split('\\n')\n",
    "            return [sentence.split() for sentence in sentences]\n",
    "\n",
    "    def _create_tokenizer(self, path: str):\n",
    "        data = self._read_all_words(path)\n",
    "        counter = Counter(data)\n",
    "        sorted_counter = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "        words, _ = list(zip(*sorted_counter))\n",
    "        word_to_id = dict(zip(words, range(1, len(words)+1)))\n",
    "        return word_to_id\n",
    "        \n",
    "    def _get_id_from_word(self, word: str) -> int:\n",
    "        return self._word_to_id.get(word, self.unk_id)\n",
    "    \n",
    "    def _sentence_to_id_list(self, sentence: List[str]) -> List[int]:\n",
    "        return [self._get_id_from_word(word) for word in sentence]\n",
    "    \n",
    "    def _get_word_from_id(self, word_id: int) -> str:\n",
    "        return self._id_to_word.get(word_id, self.unk_str)\n",
    "    \n",
    "    def _create_data(self, path: str):\n",
    "        return [self._sentence_to_id_list(sentence) for sentence in self._read_sentences(path)]\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return 0\n",
    "    \n",
    "    @property\n",
    "    def unk_id(self) -> int:\n",
    "        return self._word_to_id.get('<unk>', self.pad_id)\n",
    "    \n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._word_to_id.get('<eos>', self.pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:20.252381Z",
     "start_time": "2018-08-01T16:27:20.138061Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, config: Config, vocab_size):\n",
    "        self.config = config\n",
    "        self.vocab_size = vocab_size\n",
    "        self._create_placeholder()\n",
    "        self._create_model()\n",
    "        self.loss = self._create_loss()\n",
    "        self.accuracy = self._create_acc()\n",
    "    \n",
    "    def _create_placeholder(self):\n",
    "        self.is_training = tf.placeholder(shape=(), dtype=tf.bool, name='is_training')\n",
    "        self.inputs = tf.placeholder(shape=[None, self.config.max_length], dtype=tf.int32, name='inputs')\n",
    "        self.inputs_length = tf.placeholder(shape=[None], dtype=tf.int32, name='inputs_length')\n",
    "        self.target_ids = tf.placeholder(shape=[None], dtype=tf.int32, name='target_ids')\n",
    "    \n",
    "    def _create_model(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "        embedded_inputs = self._embedding(self.inputs)\n",
    "        _, encoder_state = self._encode(embedded_inputs)\n",
    "        # encoder_state = tf.layers.dense(encoder_state, num_units, activation=tf.nn.relu, name='hidden_layer')\n",
    "        self.outputs_logits = tf.layers.dense(encoder_state, self.vocab_size, name='outputs_layer')\n",
    "        self.predicted_id = tf.to_int32(tf.argmax(self.outputs_logits, axis=-1))\n",
    "        \n",
    "    def _create_loss(self):\n",
    "        is_target = tf.to_float(tf.not_equal(self.target_ids, 0))\n",
    "        target_ids_one_hot = tf.one_hot(self.target_ids, self.vocab_size)\n",
    "        target_ids_smoothed = self._label_smoothing(target_ids_one_hot)\n",
    "        cross_ent = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.outputs_logits, labels=target_ids_smoothed)\n",
    "        return tf.reduce_sum(cross_ent * is_target) / tf.reduce_sum(is_target)\n",
    "        \n",
    "    def _create_acc(self):\n",
    "        return tf.reduce_mean(tf.to_float(tf.equal(self.target_ids, self.predicted_id)))\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        lookup_table = tf.get_variable('lookup_table', shape=[self.vocab_size, self.config.embedding_size], dtype=tf.float32)\n",
    "        embedded_inputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        return embedded_inputs\n",
    "    \n",
    "    def _encode(self, embedded_inputs):\n",
    "        outputs, final_state = self._bidirectional_cell(\n",
    "            embedded_inputs,\n",
    "            self.config.num_layers,\n",
    "            self.config.num_units,\n",
    "            self.config.dropout_in_rate,\n",
    "            self.config.dropout_out_rate\n",
    "        )\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _bidirectional_cell(self, inputs, num_layers, num_units, dropout_in_rate, dropout_out_rate):\n",
    "        cell_fw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_fw')\n",
    "        cell_bw = self._gru(num_layers, num_units, dropout_in_rate, dropout_out_rate, name='cell_bw')\n",
    "        (fw_outputs, bw_outputs), (fw_state, bw_state) = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=cell_fw,\n",
    "            cell_bw=cell_bw,\n",
    "            inputs=inputs,\n",
    "            sequence_length=self.inputs_length,\n",
    "            dtype=tf.float32,\n",
    "            scope='bidirectional_cells')\n",
    "        outputs = tf.concat([fw_outputs, bw_outputs], axis=-1)\n",
    "        final_state = tf.reduce_sum([fw_state, bw_state], axis=0)\n",
    "        final_state = tf.concat(tf.unstack(final_state, axis=0), axis=-1)\n",
    "        return outputs, final_state\n",
    "    \n",
    "    def _gru(self, num_layers: int, num_units: int, dropout_in_rate: float, dropout_out_rate: float, name: str):\n",
    "        cells = []\n",
    "        for l in range(num_layers):\n",
    "            cell = tf.nn.rnn_cell.GRUCell(num_units, tf.nn.relu, kernel_initializer=orthogonal_initializer_without_nan, name=name)\n",
    "            if l == 0:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=1-dropout_in_rate)\n",
    "            if l == num_layers-1:\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=1-dropout_out_rate)\n",
    "            cells.append(cell)\n",
    "        return tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    \n",
    "    def _label_smoothing(self, inputs, epsilon: float=0.1):\n",
    "        feature_dim = inputs.get_shape().as_list()[-1]\n",
    "        return (1-epsilon) * inputs + (epsilon / feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:27:20.381800Z",
     "start_time": "2018-08-01T16:27:20.257422Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epoch = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-01T16:29:38.791514Z",
     "start_time": "2018-08-01T16:27:20.387053Z"
    }
   },
   "outputs": [],
   "source": [
    "for config in configs:\n",
    "    with tf.Graph().as_default():\n",
    "        now = datetime.now()\n",
    "        logdir = now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "        \n",
    "        datasource = PTBDataSource(config)\n",
    "\n",
    "        rnn = RNN(config, datasource.vocab_size)\n",
    "        optimizer = tf.train.AdamOptimizer(config.learning_rate)\n",
    "        train_op = optimizer.minimize(rnn.loss, global_step=rnn.global_step)\n",
    "        s_loss = tf.summary.scalar('loss', rnn.loss)\n",
    "        s_acc = tf.summary.scalar('accuracy', rnn.accuracy)\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver = tf.train.Saver()\n",
    "            writer = tf.summary.FileWriter(config.to_log_dir() + '/' + logdir, sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_epoch):\n",
    "                start = time.time()\n",
    "                datasource.shuffle()\n",
    "                batch_list = datasource.feed_dict_list(rnn)\n",
    "                losses = []\n",
    "                accuracies = []\n",
    "                for (j, fd) in enumerate(batch_list):\n",
    "                    loss, acc, _, smr, step = sess.run([rnn.loss, rnn.accuracy, train_op, merged_summary, rnn.global_step], feed_dict=fd)\n",
    "                    losses.append(loss)\n",
    "                    accuracies.append(acc)\n",
    "                    writer.add_summary(smr, step)\n",
    "                    #if j % 100 == 0:\n",
    "                    #    print('loss: {:.3f}, acc: {:.3f}'.format(loss, acc))\n",
    "                elapsed = time.time() - start\n",
    "                print('epoch {}/{} finished, {} step, elapsed {} sec. average loss: {:.3f}, average accuracy: {:.3f}'.format(i+1, num_epoch, step, elapsed, np.average(losses), np.average(accuracies)))\n",
    "                saver.save(sess, config.to_ckpt_path(), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
